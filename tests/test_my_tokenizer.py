from typing import Callable

import pytest
import tiktoken
import os

from minbpe import BasicTokenizer, RegexTokenizer, GPT4Tokenizer

def my_decorator_gen(need_test:bool=True):
    def my_decorator(func:Callable):
        def my_wrapper(*args, **kwargs):
            #start_time = time.time()
            print("="*30+f" {func.__name__} begin "+"="*30)
            if need_test:
                result = func(*args, **kwargs)
                #end_time = time.time()
            else:
                print(f"{func.__name__} no need to test, skip!")
                result = None
            print(f"{func.__name__} end\n\n\n")
        return my_wrapper
    return my_decorator

# -----------------------------------------------------------------------------
# common test data

# a few strings to test the tokenizers on
test_strings = [
    "", # empty string
    "?", # single character
    "hello world!!!? (ì•ˆë…•í•˜ì„¸ìš”!) lol123 ğŸ˜‰", # fun small string
    "FILE:taylorswift.txt", # FILE: is handled as a special string in unpack()
]

def unicode_test():
    print(type(ord('ä¸­')))
    print("â€˜ä¸­â€™çš„unicode code point:", ord('ä¸­'))

    print(dir())
    print(os.path.abspath(__file__))

@my_decorator_gen(need_test=True)
def test_tokenizer():
    tokenizer = BasicTokenizer()
    text = "aaabdaaabac"
    tokenizer.train(text, vocab_size=256 + 3, verbose=True)  # 256 are the byte tokens, then do 3 bigram_merge_table
    print("bigram_merge_table:")
    print(tokenizer.bigram_merge_table)
    ids = tokenizer.encode(text)
    print("ids")
    print(ids)

    print("id to token:")
    print([str(id)+":"+tokenizer.decode([id]) for id in ids])

    # [258, 100, 258, 97, 99]
    assert ids == [258, 100, 258, 97, 99]
    print(tokenizer.decode(ids))
    # aaabdaaabac
    assert (tokenizer.decode(ids)==text)

    os.makedirs("../models", exist_ok=True)
    model_name = os.path.join("../models", "toy_abc")
    tokenizer.save(model_name)

@my_decorator_gen(need_test=True)
def test_tokenizer2():
    tokenizer = BasicTokenizer()
    taylorswift_file = "taylorswift.txt"
    text = open(taylorswift_file, "r", encoding="utf-8").read()
    tokenizer.train(text, vocab_size=256 + 30, verbose=True)  # 256 are the byte tokens, then do 3 bigram_merge_table
    print("bigram_merge_table:")
    print(tokenizer.bigram_merge_table)

    text="Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. Her versatile artistry"
    ids = tokenizer.encode(text)
    print("ids:")
    print(ids)

    print("id to token:")
    print([str(id)+":"+tokenizer.decode([id]) for id in ids])

    # [258, 100, 258, 97, 99]
    expect_ids = [84, 97, 121, 108, 283, 65, 108, 105, 115, 279, 282, 266, 40, 98, 111, 114, 110, 32, 68, 101, 99, 101, 109, 98, 272, 49, 51, 257, 49, 57, 56, 57, 41, 32, 105, 262, 270, 32, 65, 109, 101, 265, 99, 270, 32, 115, 263, 103, 278, 45, 115, 264, 103, 119, 265, 116, 278, 259, 72, 272, 118, 278, 115, 97, 116, 105, 108, 256, 271, 116, 105, 115, 116, 114, 121]
    assert ids == expect_ids
    print(tokenizer.decode(ids))
    # aaabdaaabac
    assert (tokenizer.decode(ids)==text)

@my_decorator_gen(need_test=True)
def test_regex_tokenizer():
    tokenizer = RegexTokenizer()
    taylorswift_file = "taylorswift.txt"
    text = open(taylorswift_file, "r", encoding="utf-8").read()

    tokenizer.train(text, vocab_size=256 + 5, verbose=True)  # 256 are the byte tokens, then do 3 bigram_merge_table
    print("bigram_merge_table:")
    print(tokenizer.bigram_merge_table)

    text="Taylor Alison Swift (born December 13, 1989) is an American singer-songwriter. Her versatile artistry"
    ids = tokenizer.encode(text)
    print(ids)
    expect_ids = [84, 97, 121, 108, 258, 32, 65, 108, 105, 115, 111, 110, 32, 83, 119, 105, 102, 116, 32, 40, 98, 258, 110, 32, 68, 101, 99, 101, 109, 98, 256, 32, 49, 51, 44, 32, 49, 57, 56, 57, 41, 32, 105, 115, 32, 97, 110, 32, 65, 109, 256, 105, 99, 97, 110, 32, 115, 259, 103, 256, 45, 115, 111, 110, 103, 119, 114, 105, 116, 256, 46, 32, 72, 256, 32, 118, 256, 115, 97, 116, 105, 108, 101, 32, 97, 114, 116, 105, 115, 116, 114, 121]
    assert ids == expect_ids
    assert (tokenizer.decode(ids)==text)

@my_decorator_gen(need_test=True)
def test_regex_with_special():
    specials_string = """
    <|endoftext|>Hello world this is one document
    <|endoftext|>And this is another document
    <|endoftext|><|fim_prefix|>And this one has<|fim_suffix|> tokens.<|fim_middle|> FIM
    <|endoftext|>Last document!!! ğŸ‘‹<|endofprompt|>
    """.strip()
    special_tokens = {
        '<|endoftext|>': 100257,
        '<|fim_prefix|>': 100258,
        '<|fim_middle|>': 100259,
        '<|fim_suffix|>': 100260,
        '<|endofprompt|>': 100276
    }
    llama_text = """
    <|endoftext|>The llama (/ËˆlÉ‘ËmÉ™/; Spanish pronunciation: [ËˆÊama] or [ËˆÊama]) (Lama glama) is a domesticated South American camelid, widely used as a meat and pack animal by Andean cultures since the pre-Columbian era.
    Llamas are social animals and live with others as a herd. Their wool is soft and contains only a small amount of lanolin.[2] Llamas can learn simple tasks after a few repetitions. When using a pack, they can carry about 25 to 30% of their body weight for 8 to 13 km (5â€“8 miles).[3] The name llama (in the past also spelled "lama" or "glama") was adopted by European settlers from native Peruvians.[4]
    The ancestors of llamas are thought to have originated from the Great Plains of North America about 40 million years ago, and subsequently migrated to South America about three million years ago during the Great American Interchange. By the end of the last ice age (10,000â€“12,000 years ago), camelids were extinct in North America.[3] As of 2007, there were over seven million llamas and alpacas in South America and over 158,000 llamas and 100,000 alpacas, descended from progenitors imported late in the 20th century, in the United States and Canada.[5]
    <|fim_prefix|>In Aymara mythology, llamas are important beings. The Heavenly Llama is said to drink water from the ocean and urinates as it rains.[6] According to Aymara eschatology,<|fim_suffix|> where they come from at the end of time.[6]<|fim_middle|> llamas will return to the water springs and ponds<|endofprompt|>
    """.strip()

    tokenizer = RegexTokenizer()
    print("vocab size:", len(tokenizer))
    tokenizer.register_special_tokens(special_tokens)
    print("vocab size:", len(tokenizer))

    tokenizer.train(llama_text, vocab_size=256 + 100, verbose=True)  # 256 are the byte tokens, then do 3 bigram_merge_table
    print("bigram_merge_table:")
    print(tokenizer.bigram_merge_table)

    print("ids:")
    ids = tokenizer.encode(specials_string, allowed_special='all')
    print(ids)

    """
    æ³¨æ„ï¼Œæ§åˆ¶å­—ç¬¦éƒ½ä¼šæ‰“å°ä¸º:'ï¿½', è€Œæ‰€æœ‰ä¸æ§åˆ¶å­—ç¬¦ç»„æˆçš„bigramä¹Ÿå› æ­¤éƒ½ä¼šæ‰“å°ä¸º'ï¿½' 
    '33:!', '32: ', '240:ï¿½', '159:ï¿½', '145:ï¿½', '139:ï¿½', '100276:<|endofprompt|>'
    """
    print("id to token:")
    print([str(id)+":"+tokenizer.decode([id]) for id in ids])

    assert (tokenizer.decode(ids)==specials_string)

    text="<|endoftext|> and my teacher <|endofprompt|> endoftext| run.ä½ å¥½"
    ids = tokenizer.encode(text, allowed_special='all')
    print(ids)
    print(tokenizer.decode(ids))

@my_decorator_gen(need_test=True)
def test_gpt4_tokenizer():
    specials_string = """
    <|endoftext|>Hello world this is one document
    <|endoftext|>And this is another document
    <|endoftext|><|fim_prefix|>And this one has<|fim_suffix|> tokens.<|fim_middle|> FIM
    <|endoftext|>Last document!!! ğŸ‘‹<|endofprompt|>
    """.strip()
    special_tokens = {
        '<|endoftext|>': 100257,
        '<|fim_prefix|>': 100258,
        '<|fim_middle|>': 100259,
        '<|fim_suffix|>': 100260,
        '<|endofprompt|>': 100276
    }
    tokenizer = GPT4Tokenizer()
    gpt4_tokenizer_ids = tokenizer.encode(specials_string, allowed_special="all")
    #tokenizer.save_vocab("../models/gpt4.vocab")
    print(gpt4_tokenizer_ids)

@my_decorator_gen(need_test=True)
def test_regex_tokenizer_zh():
    tokenizer = RegexTokenizer()

    text = """ç”±äºã€Šè¥¿æ¸¸è®°ã€‹å‰§æƒ…èµ°å‘å¯¹å¤§éƒ¨åˆ†ä¸­å›½äººæ¥è¯´è€³ç†Ÿèƒ½è¯¦ï¼Œå†¯éª¥ä¸æ‰¬å¥‡ç­‰äººå°±å‰§æœ¬è¿›è¡Œå¤šæ¬¡è¿­ä»£ï¼Œæœ€ç»ˆå†³å®šä»¥â€œå¯»æ ¹ä¹‹æ—…â€ä¸ºæ ¸å¿ƒè¿›è¡Œå‰§æƒ…å±•å¼€ã€‚æ¸¸æˆä¸»è§’â€œå¤©å‘½äººâ€ä¸€è·¯ä¸Šä¼šé‡åˆ°å¾ˆå¤šã€Šè¥¿æ¸¸è®°ã€‹é‡Œå‡ºç°è¿‡çš„è‘—åè§’è‰²ï¼Œé€šè¿‡ä¸ä»–ä»¬æˆ˜æ–—ï¼Œæˆ–æ˜¯æˆä¸ºä¼™ä¼´ï¼Œç©å®¶å†å»å°è¯•ææ¸…æ¥šâ€œæ‚Ÿç©ºæ˜¯è°â€ä»¥åŠâ€œæˆ‘æ˜¯è°â€ [43]ã€‚
åœ¨è®¾è®¡æ¸¸æˆç¯å¢ƒè¿‡ç¨‹ä¸­ï¼Œç”±äºè‡ªç„¶æ™¯è§‚ç¼ºä¹ç°å®çš„ä¸­å¼ç´ æï¼Œå¼€å‘å›¢é˜Ÿä¸å„åœ°æ–‡ä¿éƒ¨é—¨åˆä½œå‰å¾€å®åœ°è€ƒå¯Ÿï¼Œå¯¹é™µå·äºŒä»™åº™ï¼Œæ™‹åŸé’è²å¯ºç­‰ç°å®å¤å»ºç­‘å’Œå¡‘åƒè¿›è¡Œæ‰«æï¼Œä»¥æ­¤ä¸ºè“æœ¬è¿›è¡Œé‡è§†å»ºç­‘çš„è®¾è®¡ [45]ã€‚ä¸ºäº†è®©åœºæ™¯æ›´ä¸ºé€¼çœŸï¼Œé‡‡ç”¨è™šå¹»5å¼•æ“å’ŒNVIDIAå…‰çº¿è¿½è¸ªç­‰æŠ€æœ¯ä»¥æå‡ç”»é¢æ•ˆæœ"""
    tokenizer.train(text, vocab_size=256 + 50, verbose=True)  # 256 are the byte tokens, then do 3 bigram_merge_table

    print("bigram_merge_table:")
    print(tokenizer.bigram_merge_table)
    print("vocab:")
    print(tokenizer.vocab)
    # ä¸€èˆ¬ä¸­æ–‡utf8 å 3ä¸ªbytes
    # å¯ä»¥çœ‹åˆ°æœ‰éƒ¨åˆ†ä¸­æ–‡å­—ç¼–ç ä¸ºä¸€ä¸ªæ–°token
    print("vocab decode:")
    for index, vocab_bytes in tokenizer.vocab.items():
        print(f"{index} bytes:{vocab_bytes} char:{vocab_bytes.decode('utf-8', errors='replace')}")
    """
    ç»“æœå¦‚ä¸‹ï¼š
    277 bytes:b'\xe8\xbf\x9b' char:è¿›
    278 bytes:b'\xe8\xbf\x9b\xe8' char:è¿›ï¿½
    279 bytes:b'\xe8\xbf\x9b\xe8\xa1' char:è¿›ï¿½
    280 bytes:b'\xe8\xbf\x9b\xe8\xa1\x8c' char:è¿›è¡Œ
    281 bytes:b'\xe4\xbb\xa5' char:ä»¥
    282 bytes:b'\xe2\x80\x9c' char:â€œ
    """

    text="å‰§æƒ…èµ°å‘å¯¹å¤§éƒ¨åˆ†ä¸­å›½äººæ¥è¯´è€³ç†Ÿèƒ½è¯¦"
    ids = tokenizer.encode(text)
    print(f"ids:{ids}")

if __name__ == "__main__":
    print(dir())
    if False:
        test_gpt4_tokenizer()
        test_tokenizer2()
        test_regex_tokenizer()
        unicode_test()
        test_tokenizer()
        test_regex_with_special()
    test_regex_tokenizer_zh()